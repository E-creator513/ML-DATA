{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ›’ Dagster Pipeline with Amazon Product Reviews (Colab-ready)\n",
        "\n",
        "**Purpose:** Students will gain hands-on experience in data collection, markup, cleaning, and active learning; learn how to automate the process using Dagster; and understand how to improve data using entropy and active learning metrics.\n",
        "\n",
        "Run the cells top â†’ bottom. This notebook uses `amazon_polarity` (Hugging Face dataset) as a real example of product reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run this cell only once)\n",
        "!pip install --upgrade pip --quiet\n",
        "!pip install dagster datasets scikit-learn pandas numpy matplotlib scikit-plot --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "from dagster import graph, op\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Data collection â€” why this dataset?\n",
        "\n",
        "- We use **`amazon_polarity`** because it contains real product reviews and binary sentiment labels (fast to run in Colab).\n",
        "- In practice, replace this with scraped news, API data, or images from OpenImages / Kaggle.\n",
        "- This notebook takes a **small subset** (2k rows) so it runs quickly for teaching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@op\n",
        "def collect_data_op(n_samples: int = 2000):\n",
        "    raw = load_dataset('amazon_polarity', split=f'train[:{n_samples}]')\n",
        "    df = raw.to_pandas()\n",
        "    df = df.rename(columns={'content': 'text', 'label': 'label'})\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    df['text'] = df['text'].astype(str)\n",
        "    df.to_parquet('amazon_polarity_subset.parquet', index=False)\n",
        "    return df\n",
        "\n",
        "# Quick run to collect and show data\n",
        "df_raw = collect_data_op(2000)\n",
        "print('Loaded rows:', len(df_raw))\n",
        "df_raw.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick EDA: label balance and text length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_eda(df):\n",
        "    print('Shape:', df.shape)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    df['label'].value_counts().sort_index().plot(kind='bar')\n",
        "    plt.title('Label distribution (0=neg,1=pos)')\n",
        "    plt.xlabel('Label')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n",
        "\n",
        "    df['text_len'] = df['text'].str.len()\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.hist(df['text_len'], bins=40)\n",
        "    plt.title('Text length distribution')\n",
        "    plt.show()\n",
        "\n",
        "show_eda(df_raw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Data markup â€” automatic labeling and entropy\n",
        "\n",
        "We simulate automatic markup by using the dataset's label as `auto_label`. Entropy here is a simple per-sample proxy (binary entropy of the label probability)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@op\n",
        "def auto_label_op(df: pd.DataFrame):\n",
        "    df = df.copy()\n",
        "    df['auto_label'] = df['label'].apply(lambda x: 'positive' if x==1 else 'negative')\n",
        "    p = df['label'].astype(float)\n",
        "    df['entropy'] = - (p * np.log2(p + 1e-12) + (1-p) * np.log2(1-p + 1e-12))\n",
        "    return df\n",
        "\n",
        "df_marked = auto_label_op(df_raw)\n",
        "df_marked.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(df_marked['entropy'], bins=30)\n",
        "plt.title('Entropy distribution after auto-label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Data cleanup\n",
        "\n",
        "Remove duplicates, normalize whitespace, drop too-short texts, and filter extremely long outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@op\n",
        "def clean_data_op(df: pd.DataFrame):\n",
        "    df = df.copy()\n",
        "    df = df.drop_duplicates(subset='text')\n",
        "    df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "    df = df[df['text'].str.len() > 10]\n",
        "    max_len = df['text'].str.len().quantile(0.999)\n",
        "    df = df[df['text'].str.len() <= math.ceil(max_len)]\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    return df\n",
        "\n",
        "df_clean = clean_data_op(df_marked)\n",
        "print('After cleaning rows:', len(df_clean))\n",
        "df_clean.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Active learning (uncertainty sampling) â€” sklearn-based\n",
        "\n",
        "We vectorize text with TF-IDF, train a RandomForest on a small seed, then iteratively query the most uncertain sample from the pool, add it to training, and retrain. This simulates annotator-in-the-loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@op\n",
        "def active_learning_op(df: pd.DataFrame, n_queries: int = 30):\n",
        "    df = df.copy()\n",
        "    texts = df['text'].tolist()\n",
        "    labels = df['label'].values\n",
        "\n",
        "    vec = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
        "    X_all = vec.fit_transform(texts)\n",
        "\n",
        "    # seed small training set and large pool\n",
        "    X_train, X_pool, y_train, y_pool = train_test_split(X_all, labels, test_size=0.95, random_state=42, stratify=labels)\n",
        "\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    accuracies = []\n",
        "    pool_sizes = []\n",
        "\n",
        "    queries = min(n_queries, X_pool.shape[0]-1)\n",
        "    X_train_arr = X_train.toarray() if hasattr(X_train, 'toarray') else X_train\n",
        "    X_pool_arr = X_pool.toarray()\n",
        "\n",
        "    for i in range(queries):\n",
        "        probs = model.predict_proba(X_pool_arr)\n",
        "        uncertainties = 1 - np.max(probs, axis=1)\n",
        "        query_idx = int(np.argmax(uncertainties))\n",
        "\n",
        "        X_new = X_pool_arr[query_idx].reshape(1, -1)\n",
        "        y_new = np.array([y_pool[query_idx]])\n",
        "        X_train_arr = np.vstack([X_train_arr, X_new])\n",
        "        y_train = np.append(y_train, y_new)\n",
        "\n",
        "        X_pool_arr = np.delete(X_pool_arr, query_idx, axis=0)\n",
        "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
        "\n",
        "        model.fit(X_train_arr, y_train)\n",
        "\n",
        "        y_pred_all = model.predict(X_all.toarray())\n",
        "        acc_all = accuracy_score(labels, y_pred_all)\n",
        "        accuracies.append(acc_all)\n",
        "        pool_sizes.append(X_pool_arr.shape[0])\n",
        "\n",
        "    df['active_label'] = model.predict(X_all.toarray())\n",
        "    df['active_accuracy'] = accuracies[-1] if len(accuracies)>0 else accuracy_score(labels, model.predict(X_all.toarray()))\n",
        "    return {\"df\": df, \"accuracies\": accuracies, \"pool_sizes\": pool_sizes, \"vectorizer\": vec, \"model\": model}\n",
        "\n",
        "active_result = active_learning_op(df_clean, n_queries=25)\n",
        "print('Active learning finished; last accuracy:', active_result['df']['active_accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot active learning accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracies = active_result['accuracies']\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(accuracies)+1), accuracies, marker='o')\n",
        "plt.title('Accuracy over active learning iterations')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Final training and evaluation\n",
        "\n",
        "Train a final model on the active-labeled data and show confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@op\n",
        "def train_model_op(active_result: dict):\n",
        "    df = active_result['df'].copy()\n",
        "    vec = active_result['vectorizer']\n",
        "    model = active_result['model']\n",
        "\n",
        "    X = vec.transform(df['text'].tolist()).toarray()\n",
        "    y = df['active_label'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    final_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "    final_model.fit(X_train, y_train)\n",
        "    y_pred = final_model.predict(X_test)\n",
        "    final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    df['final_predictions'] = final_model.predict(X)\n",
        "    df['final_accuracy'] = final_acc\n",
        "    return {\"df\": df, \"final_model\": final_model, \"y_test\": y_test, \"y_pred\": y_pred}\n",
        "\n",
        "final_result = train_model_op(active_result)\n",
        "print('Final test accuracy:', final_result['df']['final_accuracy'].iloc[0] if 'final_accuracy' in final_result['df'].columns else final_result['y_pred'].shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "y_test = final_result['y_test']\n",
        "y_pred = final_result['y_pred']\n",
        "plt.figure(figsize=(6,6))\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.title(f\"Final model confusion matrix (acc={final_result['df']['final_accuracy'].iloc[0]:.3f})\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save outputs\n",
        "\n",
        "Saved processed dataset and final result for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df = final_result['df']\n",
        "final_df.to_parquet('amazon_pipeline_result.parquet', index=False)\n",
        "print('Saved amazon_pipeline_result.parquet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes for instructors\n",
        "\n",
        "- The active learning loop uses the true labels from the pool as a simulated oracle. In real setups, you would push queried samples to an annotation UI (Label Studio) and ingest labels back.\n",
        "- To adapt for images, swap TF-IDF + RF for CNN features and use YOLO/Detectron for object markup.\n",
        "- Encourage students to swap the dataset in `collect_data_op` with scraped news, a Kaggle CSV, or an S3/DB source."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}